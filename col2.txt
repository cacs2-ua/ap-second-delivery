# CNN Architectures

In this notebook, we are going to implement a state-of-the-art convolutional neural network. You can use PyTorch, Keras, or TensorFlow for this purpose. However, you should provide a summary of the created network at the end. You have to implement an architecture between AlexNet and VGG19 and another architecture between Resnet50 and Inception v1.


#@title Example of Summary

# print(model)

VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): ReLU(inplace=True)
    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (24): ReLU(inplace=True)
    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (26): ReLU(inplace=True)
    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): ReLU(inplace=True)
    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (33): ReLU(inplace=True)
    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (35): ReLU(inplace=True)
    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)

resnet50

ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
...
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...


inceptionv3

Inception3(
  (Conv2d_1a_3x3): BasicConv2d(
    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)
    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
  )
  (Conv2d_2a_3x3): BasicConv2d(
    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
  )
  (Conv2d_2b_3x3): BasicConv2d(
    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  (Conv2d_3b_1x1): BasicConv2d(
    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
  )
  (Conv2d_4a_3x3): BasicConv2d(
    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  (Mixed_5b): InceptionA(
    (branch1x1): BasicConv2d(
...
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...

## **First Implementation:**


# =========================
# First Implementation
# Architecture between AlexNet and VGG19 (PyTorch)
# =========================

import torch
from torch import nn
import torch.nn.functional as F


class AlexVGGNet(nn.Module):
    """
    A hybrid CNN:
      - AlexNet-like stem (large first conv + early pooling)
      - VGG-style stacks of 3x3 convolutions (deeper than AlexNet, shallower than VGG19)
      - VGG-like classifier head
    """

    def __init__(self, num_classes: int = 1000):
        super().__init__()

        # AlexNet-like stem
        self.stem = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )

        # VGG-like blocks (3x3 conv stacks + maxpool)
        self.features = nn.Sequential(
            # Block 1: 64 -> 64 (2 convs)
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            # Block 2: 64 -> 128 (2 convs)
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            # Block 3: 128 -> 256 (3 convs)
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            # Block 4: 256 -> 512 (2 convs)
            nn.Conv2d(256, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            # Block 5: 512 -> 512 (2 convs)
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )

        # Same style as VGG summary shown in the statement
        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))
        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5),
            nn.Linear(4096, num_classes),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.zeros_(m.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.stem(x)
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x


# Instantiate the first model
model_first = AlexVGGNet(num_classes=1000)


## **Second Implementation:**

# =========================
# Second Implementation
# Architecture between ResNet50 and Inception v1 (PyTorch)
# =========================

class ConvBNReLU(nn.Module):
    def __init__(self, in_ch: int, out_ch: int, k: int, s: int = 1, p: int = 0):
        super().__init__()
        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=k, stride=s, padding=p, bias=False)
        self.bn = nn.BatchNorm2d(out_ch)
        self.act = nn.ReLU(inplace=True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.act(self.bn(self.conv(x)))


class InceptionV1Module(nn.Module):
    """
    Inception v1 style:
      - 1x1
      - 1x1 -> 3x3
      - 1x1 -> 5x5
      - 3x3 maxpool -> 1x1
    All branches keep spatial size (padding where needed), then concat on channels.
    """

    def __init__(
        self,
        in_ch: int,
        c1: int,
        c2_reduce: int, c2: int,
        c3_reduce: int, c3: int,
        c4: int,
    ):
        super().__init__()
        # Branch 1: 1x1
        self.b1 = ConvBNReLU(in_ch, c1, k=1, s=1, p=0)

        # Branch 2: 1x1 -> 3x3
        self.b2 = nn.Sequential(
            ConvBNReLU(in_ch, c2_reduce, k=1, s=1, p=0),
            ConvBNReLU(c2_reduce, c2, k=3, s=1, p=1),
        )

        # Branch 3: 1x1 -> 5x5
        self.b3 = nn.Sequential(
            ConvBNReLU(in_ch, c3_reduce, k=1, s=1, p=0),
            ConvBNReLU(c3_reduce, c3, k=5, s=1, p=2),
        )

        # Branch 4: pool -> 1x1
        self.b4_pool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
        self.b4 = ConvBNReLU(in_ch, c4, k=1, s=1, p=0)

        self.out_channels = c1 + c2 + c3 + c4

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        y1 = self.b1(x)
        y2 = self.b2(x)
        y3 = self.b3(x)
        y4 = self.b4(self.b4_pool(x))
        return torch.cat([y1, y2, y3, y4], dim=1)


class InceptionResBlock(nn.Module):
    """
    Residual wrapper around an Inception v1 module:
      y = ReLU( Inception(x) * scale + projection(x) )
    """

    def __init__(self, in_ch: int, inception_kwargs: dict, scale: float = 0.1):
        super().__init__()
        self.scale = scale
        self.inception = InceptionV1Module(in_ch, **inception_kwargs)
        out_ch = self.inception.out_channels

        if in_ch != out_ch:
            self.proj = nn.Sequential(
                nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False),
                nn.BatchNorm2d(out_ch),
            )
        else:
            self.proj = nn.Identity()

        self.act = nn.ReLU(inplace=True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        shortcut = self.proj(x)
        out = self.inception(x)
        out = out * self.scale + shortcut
        return self.act(out)


class ResInceptionNet(nn.Module):
    """
    A network mixing:
      - ResNet-like stem
      - Inception v1 modules inside residual blocks (Inception-Res style)
    """

    def __init__(self, num_classes: int = 1000):
        super().__init__()

        # ResNet-like stem
        self.stem = nn.Sequential(
            ConvBNReLU(3, 64, k=7, s=2, p=3),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
        )

        # Stage 1 (keep resolution)
        self.stage1 = nn.Sequential(
            InceptionResBlock(64, dict(c1=64, c2_reduce=64, c2=96, c3_reduce=16, c3=32, c4=32)),
            InceptionResBlock(224, dict(c1=64, c2_reduce=64, c2=96, c3_reduce=16, c3=32, c4=32)),
        )
        # After stage1, channels = 64+96+32+32 = 224

        # Downsample to stage2
        self.down1 = nn.Sequential(
            ConvBNReLU(224, 320, k=3, s=2, p=1),
        )

        # Stage 2
        self.stage2 = nn.Sequential(
            InceptionResBlock(320, dict(c1=96, c2_reduce=96, c2=128, c3_reduce=16, c3=48, c4=48)),
            InceptionResBlock(320, dict(c1=96, c2_reduce=96, c2=128, c3_reduce=16, c3=48, c4=48)),
            InceptionResBlock(320, dict(c1=96, c2_reduce=96, c2=128, c3_reduce=16, c3=48, c4=48)),
        )
        # stage2 output channels = 96+128+48+48 = 320

        # Downsample to stage3
        self.down2 = nn.Sequential(
            ConvBNReLU(320, 512, k=3, s=2, p=1),
        )

        # Stage 3
        self.stage3 = nn.Sequential(
            InceptionResBlock(512, dict(c1=128, c2_reduce=128, c2=192, c3_reduce=32, c3=64, c4=128)),
            InceptionResBlock(512, dict(c1=128, c2_reduce=128, c2=192, c3_reduce=32, c3=64, c4=128)),
            InceptionResBlock(512, dict(c1=128, c2_reduce=128, c2=192, c3_reduce=32, c3=64, c4=128)),
            InceptionResBlock(512, dict(c1=128, c2_reduce=128, c2=192, c3_reduce=32, c3=64, c4=128)),
        )
        # stage3 output channels = 128+192+64+128 = 512

        # Downsample to stage4
        self.down3 = nn.Sequential(
            ConvBNReLU(512, 768, k=3, s=2, p=1),
        )

        # Stage 4 (final)
        self.stage4 = nn.Sequential(
            InceptionResBlock(768, dict(c1=192, c2_reduce=160, c2=256, c3_reduce=32, c3=96, c4=224)),
            InceptionResBlock(768, dict(c1=192, c2_reduce=160, c2=256, c3_reduce=32, c3=96, c4=224)),
        )
        # stage4 output channels = 192+256+96+224 = 768

        # Head
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout(p=0.2)
        self.fc = nn.Linear(768, num_classes)

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.zeros_(m.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.stem(x)
        x = self.stage1(x)
        x = self.down1(x)
        x = self.stage2(x)
        x = self.down2(x)
        x = self.stage3(x)
        x = self.down3(x)
        x = self.stage4(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        x = self.fc(x)
        return x


# Instantiate the second model
model_second = ResInceptionNet(num_classes=1000)


## Auxiliary code to check the created model.


from torchvision import models
alexnet = models.alexnet(pretrained=True)
vgg19 = models.vgg19(pretrained=True)
resnet50 = models.resnet50(pretrained=True)
inceptionv3 = models.inception_v3(pretrained=True)


import io
import sys

def capture_model_print(model):
    old_stdout = sys.stdout
    sys.stdout = buffer = io.StringIO()
    print(model)
    sys.stdout = old_stdout
    return buffer.getvalue()

model = model_second 
custom_model = capture_model_print(model)
pytorch_model = capture_model_print(vgg19)

if custom_model == pytorch_model:
    print("The models have identical structures.")
else:
    print("The models structures differ.")


# ============================================================
# Sanity-check cell for:
#   (1) First implementation: AlexVGGNet (fixed to work on 224/227 inputs)
#   (2) Second implementation: ResInceptionNet (as provided)
#
# What this cell does:
#   - Builds both models (1000 classes + 3 classes variants)
#   - Runs forward-shape tests on common input sizes
#   - Runs a backward/gradient test (one step)
#   - Prints parameter counts and checks they are "between" reference models
#   - Prints model summaries
# ============================================================

import random
import numpy as np
import torch
from torch import nn
import torch.nn.functional as F

# ----------------------------
# Reproducibility + device
# ----------------------------
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ============================================================
# 1) FIRST IMPLEMENTATION (FIX)
# ============================================================
# Reason for the fix:
# Your original AlexVGGNet downsamples too aggressively:
#   stem (stride=4 + pool stride=2) + 5 additional pools
# => spatial size collapses to 0 for 224/227 inputs.
#
# Fix approach (minimal + principled):
# - Keep AlexNet-like stem (11x11, stride 4, early pooling).
# - Use only TWO additional 2x2 pooling operations (AlexNet-style),
#   so that 224/227 end up at 6x6 feature maps (like AlexNet),
#   and then use AdaptiveAvgPool2d((6,6)).
# - Keep VGG-ish conv stacks (more conv layers than AlexNet, fewer than VGG19).
# ============================================================

class AlexVGGNet(nn.Module):
    """
    Hybrid CNN:
      - AlexNet-like stem (11x11, stride 4 + early pooling)
      - VGG-style 3x3 conv stacks
      - AlexNet-style spatial resolution before classifier (6x6)
    Works reliably for input sizes 224 and 227.
    """
    def __init__(self, num_classes: int = 1000):
        super().__init__()

        # AlexNet-like stem
        self.stem = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),   # -> ~27x27
        )

        # VGG-like conv stacks (ONLY TWO pools total here to avoid collapse)
        self.features = nn.Sequential(
            # Block 1: 64 -> 64 (2 convs) + pool
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),   # 27 -> 13

            # Block 2: 64 -> 128 (2 convs) + pool
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),   # 13 -> 6

            # Block 3: 128 -> 256 (3 convs)
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),

            # Block 4: 256 -> 512 (2 convs)
            nn.Conv2d(256, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),

            # Block 5: 512 -> 512 (2 convs)
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
        )

        # Ensure a stable classifier input like AlexNet (6x6)
        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))
        self.classifier = nn.Sequential(
            nn.Linear(512 * 6 * 6, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5),
            nn.Linear(4096, num_classes),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.zeros_(m.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.stem(x)
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        return self.classifier(x)


# ============================================================
# 2) SECOND IMPLEMENTATION (AS PROVIDED)
# ============================================================

class ConvBNReLU(nn.Module):
    def __init__(self, in_ch: int, out_ch: int, k: int, s: int = 1, p: int = 0):
        super().__init__()
        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=k, stride=s, padding=p, bias=False)
        self.bn = nn.BatchNorm2d(out_ch)
        self.act = nn.ReLU(inplace=True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.act(self.bn(self.conv(x)))


class InceptionV1Module(nn.Module):
    """
    Inception v1 style branches:
      - 1x1
      - 1x1 -> 3x3
      - 1x1 -> 5x5
      - 3x3 maxpool -> 1x1
    """
    def __init__(self, in_ch: int, c1: int, c2_reduce: int, c2: int, c3_reduce: int, c3: int, c4: int):
        super().__init__()
        self.b1 = ConvBNReLU(in_ch, c1, k=1, s=1, p=0)
        self.b2 = nn.Sequential(
            ConvBNReLU(in_ch, c2_reduce, k=1, s=1, p=0),
            ConvBNReLU(c2_reduce, c2, k=3, s=1, p=1),
        )
        self.b3 = nn.Sequential(
            ConvBNReLU(in_ch, c3_reduce, k=1, s=1, p=0),
            ConvBNReLU(c3_reduce, c3, k=5, s=1, p=2),
        )
        self.b4_pool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
        self.b4 = ConvBNReLU(in_ch, c4, k=1, s=1, p=0)
        self.out_channels = c1 + c2 + c3 + c4

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        y1 = self.b1(x)
        y2 = self.b2(x)
        y3 = self.b3(x)
        y4 = self.b4(self.b4_pool(x))
        return torch.cat([y1, y2, y3, y4], dim=1)


class InceptionResBlock(nn.Module):
    """
    Residual wrapper around an Inception v1 module:
      y = ReLU( inception(x) * scale + projection(x) )
    """
    def __init__(self, in_ch: int, inception_kwargs: dict, scale: float = 0.1):
        super().__init__()
        self.scale = scale
        self.inception = InceptionV1Module(in_ch, **inception_kwargs)
        out_ch = self.inception.out_channels

        if in_ch != out_ch:
            self.proj = nn.Sequential(
                nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False),
                nn.BatchNorm2d(out_ch),
            )
        else:
            self.proj = nn.Identity()

        self.act = nn.ReLU(inplace=True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        shortcut = self.proj(x)
        out = self.inception(x)
        out = out * self.scale + shortcut
        return self.act(out)


class ResInceptionNet(nn.Module):
    """
    A network mixing:
      - ResNet-like stem
      - Inception v1 modules inside residual blocks (Inception-Res style)
    """
    def __init__(self, num_classes: int = 1000):
        super().__init__()
        self.stem = nn.Sequential(
            ConvBNReLU(3, 64, k=7, s=2, p=3),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
        )

        self.stage1 = nn.Sequential(
            InceptionResBlock(64, dict(c1=64, c2_reduce=64, c2=96, c3_reduce=16, c3=32, c4=32)),
            InceptionResBlock(224, dict(c1=64, c2_reduce=64, c2=96, c3_reduce=16, c3=32, c4=32)),
        )
        self.down1 = nn.Sequential(ConvBNReLU(224, 320, k=3, s=2, p=1))

        self.stage2 = nn.Sequential(
            InceptionResBlock(320, dict(c1=96, c2_reduce=96, c2=128, c3_reduce=16, c3=48, c4=48)),
            InceptionResBlock(320, dict(c1=96, c2_reduce=96, c2=128, c3_reduce=16, c3=48, c4=48)),
            InceptionResBlock(320, dict(c1=96, c2_reduce=96, c2=128, c3_reduce=16, c3=48, c4=48)),
        )
        self.down2 = nn.Sequential(ConvBNReLU(320, 512, k=3, s=2, p=1))

        self.stage3 = nn.Sequential(
            InceptionResBlock(512, dict(c1=128, c2_reduce=128, c2=192, c3_reduce=32, c3=64, c4=128)),
            InceptionResBlock(512, dict(c1=128, c2_reduce=128, c2=192, c3_reduce=32, c3=64, c4=128)),
            InceptionResBlock(512, dict(c1=128, c2_reduce=128, c2=192, c3_reduce=32, c3=64, c4=128)),
            InceptionResBlock(512, dict(c1=128, c2_reduce=128, c2=192, c3_reduce=32, c3=64, c4=128)),
        )
        self.down3 = nn.Sequential(ConvBNReLU(512, 768, k=3, s=2, p=1))

        self.stage4 = nn.Sequential(
            InceptionResBlock(768, dict(c1=192, c2_reduce=160, c2=256, c3_reduce=32, c3=96, c4=224)),
            InceptionResBlock(768, dict(c1=192, c2_reduce=160, c2=256, c3_reduce=32, c3=96, c4=224)),
        )

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout(p=0.2)
        self.fc = nn.Linear(768, num_classes)

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.zeros_(m.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.stem(x)
        x = self.stage1(x)
        x = self.down1(x)
        x = self.stage2(x)
        x = self.down2(x)
        x = self.stage3(x)
        x = self.down3(x)
        x = self.stage4(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        return self.fc(x)


# ============================================================
# Helpers for tests
# ============================================================

def count_params(model: nn.Module) -> int:
    return sum(p.numel() for p in model.parameters())

def human(n: int) -> str:
    if n >= 1_000_000:
        return f"{n/1_000_000:.2f}M"
    if n >= 1_000:
        return f"{n/1_000:.2f}K"
    return str(n)

@torch.no_grad()
def forward_shape_test(model: nn.Module, input_shape=(2, 3, 224, 224), num_classes_expected=1000, name="model"):
    model = model.to(device).eval()
    x = torch.randn(*input_shape, device=device)
    y = model(x)
    assert y.ndim == 2, f"[{name}] Output must be 2D (N,C). Got {tuple(y.shape)}"
    assert y.shape[0] == input_shape[0], f"[{name}] Batch mismatch. Expected {input_shape[0]}, got {y.shape[0]}"
    assert y.shape[1] == num_classes_expected, f"[{name}] Class dim mismatch. Expected {num_classes_expected}, got {y.shape[1]}"
    assert torch.isfinite(y).all(), f"[{name}] Output has NaN/Inf."
    print(f"[PASS] {name}: forward ok -> output shape {tuple(y.shape)}")

def backward_test(model: nn.Module, num_classes=1000, input_shape=(2, 3, 224, 224), name="model"):
    model = model.to(device).train()
    x = torch.randn(*input_shape, device=device)
    targets = torch.randint(0, num_classes, (input_shape[0],), device=device)
    y = model(x)
    loss = F.cross_entropy(y, targets)
    loss.backward()

    # Ensure at least one trainable parameter got a finite gradient
    finite_grads = 0
    for p in model.parameters():
        if p.requires_grad and p.grad is not None and torch.isfinite(p.grad).all():
            finite_grads += 1
            break
    assert finite_grads > 0, f"[{name}] No finite gradients found."
    print(f"[PASS] {name}: backward ok -> loss={loss.item():.4f}")

def safe_torchvision_model(builder, **kwargs):
    """
    Avoids downloads and handles torchvision API differences:
      - New API: builder(weights=None, ...)
      - Old API: builder(pretrained=False, ...)
    """
    try:
        return builder(weights=None, **kwargs)
    except TypeError:
        # Older torchvision
        return builder(pretrained=False, **kwargs)


# ============================================================
# Build custom models (1000 and 3 classes)
# ============================================================
model_first_1000 = AlexVGGNet(num_classes=1000)
model_first_3 = AlexVGGNet(num_classes=3)

model_second_1000 = ResInceptionNet(num_classes=1000)
model_second_3 = ResInceptionNet(num_classes=3)

# ============================================================
# Reference models (no pretrained weights => no downloads)
# ============================================================
from torchvision import models

alexnet_ref = safe_torchvision_model(models.alexnet)
vgg19_ref = safe_torchvision_model(models.vgg19)
resnet50_ref = safe_torchvision_model(models.resnet50)

# Inception v1 in torchvision is GoogLeNet
googlenet_ref = safe_torchvision_model(models.googlenet)

# ============================================================
# Parameter count checks
# ============================================================
p_first = count_params(model_first_1000)
p_alex = count_params(alexnet_ref)
p_vgg = count_params(vgg19_ref)

p_second = count_params(model_second_1000)
p_google = count_params(googlenet_ref)
p_res50 = count_params(resnet50_ref)

print("\n=== Parameter counts ===")
print(f"AlexVGGNet (custom): {human(p_first)} params")
print(f"AlexNet (ref):       {human(p_alex)} params")
print(f"VGG19 (ref):         {human(p_vgg)} params")

print(f"\nResInceptionNet (custom): {human(p_second)} params")
print(f"GoogLeNet / Inception v1 (ref): {human(p_google)} params")
print(f"ResNet50 (ref):               {human(p_res50)} params")

print("\n=== 'Between' sanity checks (by parameter count) ===")
if p_alex < p_first < p_vgg:
    print("[PASS] First model params are between AlexNet and VGG19.")
else:
    print("[WARN] First model params are NOT between AlexNet and VGG19 (this may still be acceptable conceptually).")

if p_google < p_second < p_res50:
    print("[PASS] Second model params are between Inception v1 (GoogLeNet) and ResNet50.")
else:
    print("[WARN] Second model params are NOT between Inception v1 (GoogLeNet) and ResNet50 (this may still be acceptable conceptually).")

# ============================================================
# Forward tests
# ============================================================
print("\n=== Forward shape tests ===")
forward_shape_test(model_first_1000, input_shape=(2, 3, 224, 224), num_classes_expected=1000, name="AlexVGGNet @224")
forward_shape_test(model_first_1000, input_shape=(2, 3, 227, 227), num_classes_expected=1000, name="AlexVGGNet @227")
forward_shape_test(model_first_3,    input_shape=(2, 3, 224, 224), num_classes_expected=3,    name="AlexVGGNet(3cls) @224")

forward_shape_test(model_second_1000, input_shape=(2, 3, 224, 224), num_classes_expected=1000, name="ResInceptionNet @224")
forward_shape_test(model_second_3,    input_shape=(2, 3, 224, 224), num_classes_expected=3,    name="ResInceptionNet(3cls) @224")

# ============================================================
# Backward tests (gradients)
# ============================================================
print("\n=== Backward / gradient tests ===")
backward_test(model_first_1000, num_classes=1000, input_shape=(2, 3, 224, 224), name="AlexVGGNet @224")
backward_test(model_second_1000, num_classes=1000, input_shape=(2, 3, 224, 224), name="ResInceptionNet @224")

# ============================================================
# Print summaries (as required by the assignment)
# ============================================================
print("\n=== Model summaries ===")
print("\n--- First implementation (AlexVGGNet) ---")
print(model_first_1000)

print("\n--- Second implementation (ResInceptionNet) ---")
print(model_second_1000)

print("\n✅ ALL SANITY CHECKS COMPLETED (see PASS/WARN messages above).")

# Dataset and Training

Now we are going to train one of the implemented networks. For this, we will use the CIFAR-10 dataset. This dataset is composed of 10 classes but we will only use 3.


#@title Imports
import glob
import random
import numpy as np
import pandas as pd

import torch
from torch import nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset

torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


#@title Dataset

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize to [-1, 1] range
])

def filter_indices_with_limits(dataset, class_sample_limits):
    indices = []
    class_counts = {cls: 0 for cls in class_sample_limits.keys()}
    for i in range(len(dataset)):
        _, label = dataset[i]
        class_name = dataset.classes[label]
        if class_name in class_sample_limits:
            if class_counts[class_name] < class_sample_limits[class_name]:
                indices.append(i)
                class_counts[class_name] += 1
            if all(class_counts[cls] >= limit for cls, limit in class_sample_limits.items()):
                break
    return indices


trainset = datasets.CIFAR10(root='CIFAR10_data/', train=True, download=True, transform=transform)
testset = datasets.CIFAR10(root='CIFAR10_data/', train=False, download=True, transform=transform)

train_limits = {'airplane': 500, 'automobile': 500, 'bird': 50}
test_limits = {'airplane': 100, 'automobile': 100, 'bird': 100}

train_indices = filter_indices_with_limits(trainset, train_limits)
trainset_filtered = Subset(trainset, train_indices)

test_indices = filter_indices_with_limits(testset, test_limits)
testset_filtered = Subset(testset, test_indices)

trainloader = DataLoader(trainset_filtered, batch_size=64, shuffle=True)
testloader = DataLoader(testset_filtered, batch_size=64, shuffle=True)


## Data Augmentation

As you can see, we have a dataset that is not balanced. The `bird` class only has 50 samples, while `airplane` and `automobile` have 500. In this section, you will need to implement a data augmentation technique to solve this problem in our dataset.

You can find examples of different techniques at the following link:

1. [Getting started with transforms v2](https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_getting_started.html#sphx-glr-auto-examples-transforms-plot-transforms-getting-started-py)
2. [Transforming and augmenting images](https://pytorch.org/vision/stable/transforms.html)


### **Apply Data Augmentation**

# --- Apply Data Augmentation (build UNBALANCED + BALANCED loaders, and TEST loader) ---

import math
import random
import numpy as np
import torch
from torch.utils.data import Dataset, Subset, ConcatDataset, DataLoader
from torchvision.transforms import v2
from torchvision import datasets

# -------------------------
# Reproducibility
# -------------------------
def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(42)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# IMPORTANT (Windows/Jupyter):
# num_workers>0 can raise PicklingError with custom Dataset wrappers.
NUM_WORKERS = 0
PIN_MEMORY = torch.cuda.is_available()

BATCH_SIZE = 64

# -------------------------
# Load CIFAR-10 RAW (no transforms)
# -------------------------
trainset_raw = datasets.CIFAR10(root="CIFAR10_data/", train=True,  download=True, transform=None)
testset_raw  = datasets.CIFAR10(root="CIFAR10_data/", train=False, download=True, transform=None)

# -------------------------
# Filtering function (same as practice)
# -------------------------
def filter_indices_with_limits(dataset, class_sample_limits):
    indices = []
    class_counts = {cls: 0 for cls in class_sample_limits.keys()}
    for i in range(len(dataset)):
        _, label = dataset[i]
        class_name = dataset.classes[label]
        if class_name in class_sample_limits:
            if class_counts[class_name] < class_sample_limits[class_name]:
                indices.append(i)
                class_counts[class_name] += 1
            if all(class_counts[cls] >= limit for cls, limit in class_sample_limits.items()):
                break
    return indices

train_limits = {"airplane": 500, "automobile": 500, "bird": 50}
test_limits  = {"airplane": 100, "automobile": 100, "bird": 100}

train_indices = filter_indices_with_limits(trainset_raw, train_limits)
test_indices  = filter_indices_with_limits(testset_raw,  test_limits)

# -------------------------
# v2 transforms
# Keep practice normalization to [-1,1] (mean=0.5,std=0.5)
# -------------------------
eval_preprocess = v2.Compose([
    v2.ToImage(),
    v2.Resize((224, 224), antialias=True),
    v2.ToDtype(torch.float32, scale=True),
    v2.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
])

# Mild but strong general-purpose augmentation (applied to ALL classes for training in BOTH setups)
# This increases accuracy without changing the meaning of "unbalanced" (it stays imbalanced in counts).
train_base_aug = v2.Compose([
    v2.ToImage(),
    v2.RandomResizedCrop(size=(224, 224), scale=(0.75, 1.0), ratio=(0.80, 1.25), antialias=True),
    v2.RandomHorizontalFlip(p=0.5),
    v2.TrivialAugmentWide(),  # strong, simple, often boosts accuracy
    v2.ToDtype(torch.float32, scale=True),
    v2.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
    v2.RandomErasing(p=0.10, scale=(0.02, 0.18), ratio=(0.3, 3.3), value="random"),
])

# Stronger augmentation ONLY for the oversampled "bird" copies (keeps requirement: "balance by augmented oversampling")
bird_strong_aug = v2.Compose([
    v2.ToImage(),
    v2.RandomResizedCrop(size=(224, 224), scale=(0.65, 1.0), ratio=(0.75, 1.33), antialias=True),
    v2.RandomHorizontalFlip(p=0.5),
    v2.RandomRotation(degrees=18),
    v2.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.06),
    v2.RandAugment(num_ops=2, magnitude=9),
    v2.ToDtype(torch.float32, scale=True),
    v2.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
    v2.RandomErasing(p=0.25, scale=(0.02, 0.20), ratio=(0.3, 3.3), value="random"),
])

class TransformOnlyImage(Dataset):
    """Wrap a dataset/subset and apply a transform ONLY to the image (label unchanged)."""
    def __init__(self, subset: Dataset, transform: torch.nn.Module):
        self.subset = subset
        self.transform = transform

    def __len__(self):
        return len(self.subset)

    def __getitem__(self, idx):
        img, label = self.subset[idx]
        img = self.transform(img)
        return img, label

# -------------------------
# Build UNBALANCED training dataset (counts: 500 / 500 / 50)
# -------------------------
trainset_unbalanced = TransformOnlyImage(Subset(trainset_raw, train_indices), train_base_aug)

# -------------------------
# Build BALANCED training dataset by augmented oversampling of "bird" up to majority (500)
# -------------------------
cls_to_idx = trainset_raw.class_to_idx
airplane_id   = cls_to_idx["airplane"]
automobile_id = cls_to_idx["automobile"]
bird_id       = cls_to_idx["bird"]

airplane_indices   = [i for i in train_indices if trainset_raw.targets[i] == airplane_id]
automobile_indices = [i for i in train_indices if trainset_raw.targets[i] == automobile_id]
bird_indices       = [i for i in train_indices if trainset_raw.targets[i] == bird_id]

target_per_class = max(len(airplane_indices), len(automobile_indices))  # should be 500
bird_needed = target_per_class - len(bird_indices)                      # should be 450

if bird_needed > 0:
    reps = math.ceil(bird_needed / max(1, len(bird_indices)))
    bird_extra_indices = (bird_indices * reps)[:bird_needed]
else:
    bird_extra_indices = []

train_airplane_ds   = TransformOnlyImage(Subset(trainset_raw, airplane_indices),   train_base_aug)
train_automobile_ds = TransformOnlyImage(Subset(trainset_raw, automobile_indices), train_base_aug)
train_bird_orig_ds  = TransformOnlyImage(Subset(trainset_raw, bird_indices),       train_base_aug)

datasets_to_concat = [train_airplane_ds, train_automobile_ds, train_bird_orig_ds]

if len(bird_extra_indices) > 0:
    train_bird_aug_ds = TransformOnlyImage(Subset(trainset_raw, bird_extra_indices), bird_strong_aug)
    datasets_to_concat.append(train_bird_aug_ds)

trainset_balanced = ConcatDataset(datasets_to_concat)

# -------------------------
# TEST dataset (no augmentation)
# -------------------------
testset_eval = TransformOnlyImage(Subset(testset_raw, test_indices), eval_preprocess)

# -------------------------
# DataLoaders (NOTE shuffle=False for test)
# -------------------------
trainloader_unbalanced = DataLoader(
    trainset_unbalanced,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=NUM_WORKERS,
    pin_memory=PIN_MEMORY,
    drop_last=True
)

trainloader_balanced = DataLoader(
    trainset_balanced,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=NUM_WORKERS,
    pin_memory=PIN_MEMORY,
    drop_last=True
)

testloader = DataLoader(
    testset_eval,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=NUM_WORKERS,
    pin_memory=PIN_MEMORY
)

class_names = ["airplane", "automobile", "bird"]
NUM_CLASSES = 3

print("Loaders ready:")
print(f"  Unbalanced train size: {len(trainset_unbalanced)}")
print(f"  Balanced train size:   {len(trainset_balanced)}")
print(f"  Test size:             {len(testset_eval)}")


# ============================================================
# Sanity-check code for: "Apply Data Augmentation"
# (balanced dataset by augmented oversampling of the "bird" class)
# ============================================================

import os, math, random
import torch
from torch.utils.data import Dataset, Subset, ConcatDataset, DataLoader
from torchvision import datasets

# --- torchvision.transforms.v2 availability check ---
try:
    from torchvision.transforms import v2
except Exception as e:
    raise RuntimeError(
        "torchvision.transforms.v2 is not available in your environment. "
        "Please upgrade torchvision (e.g., torchvision>=0.15) and retry."
    ) from e

# -----------------------------
# 1) Helper: filter indices with per-class limits
# -----------------------------
def filter_indices_with_limits(cifar_dataset, limits: dict):
    """
    Returns a list of indices that keeps ONLY the classes present in `limits`,
    and caps each class to at most limits[class_name] samples.
    """
    cls_to_idx = cifar_dataset.class_to_idx
    wanted_ids = {cls_to_idx[name]: cap for name, cap in limits.items()}

    counts = {k: 0 for k in wanted_ids.keys()}
    out = []
    for i, y in enumerate(cifar_dataset.targets):
        if y in wanted_ids and counts[y] < wanted_ids[y]:
            out.append(i)
            counts[y] += 1

        # Early stop if all caps reached
        if all(counts[k] >= wanted_ids[k] for k in wanted_ids):
            break
    return out

# -----------------------------
# 2) Define limits so "bird" is the minority (to force augmentation)
#    Adjust these to match your notebook if you want.
# -----------------------------
train_limits = {"airplane": 500, "automobile": 500, "bird": 200}
test_limits  = {"airplane": 100, "automobile": 100, "bird": 100}

# -----------------------------
# 3) (Re)load CIFAR-10 WITHOUT transforms so we can apply transforms.v2 cleanly
# -----------------------------
os.makedirs("CIFAR10_data/", exist_ok=True)
trainset_raw = datasets.CIFAR10(root="CIFAR10_data/", train=True,  download=True, transform=None)
testset_raw  = datasets.CIFAR10(root="CIFAR10_data/", train=False, download=True, transform=None)

# -----------------------------
# 4) v2 transforms: base preprocessing (no randomness) + augmentation pipeline
# -----------------------------
base_preprocess = v2.Compose([
    v2.ToImage(),
    v2.Resize((224, 224), antialias=True),
    v2.ToDtype(torch.float32, scale=True),
    v2.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
])

bird_augment = v2.Compose([
    v2.ToImage(),
    v2.RandomResizedCrop(size=(224, 224), scale=(0.7, 1.0), ratio=(0.75, 1.33), antialias=True),
    v2.RandomHorizontalFlip(p=0.5),
    v2.RandomRotation(degrees=15),
    v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),
    v2.RandomErasing(p=0.25, scale=(0.02, 0.15), ratio=(0.3, 3.3), value="random"),
    v2.ToDtype(torch.float32, scale=True),
    v2.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
])

class TransformOnlyImage(Dataset):
    def __init__(self, subset: Dataset, transform: torch.nn.Module):
        self.subset = subset
        self.transform = transform

    def __len__(self):
        return len(self.subset)

    def __getitem__(self, idx):
        img, label = self.subset[idx]
        img = self.transform(img)
        return img, label

# -----------------------------
# 5) Recompute filtered indices and split by class
# -----------------------------
train_indices = filter_indices_with_limits(trainset_raw, train_limits)
test_indices  = filter_indices_with_limits(testset_raw,  test_limits)

cls_to_idx = trainset_raw.class_to_idx
airplane_id   = cls_to_idx["airplane"]
automobile_id = cls_to_idx["automobile"]
bird_id       = cls_to_idx["bird"]

airplane_indices   = [i for i in train_indices if trainset_raw.targets[i] == airplane_id]
automobile_indices = [i for i in train_indices if trainset_raw.targets[i] == automobile_id]
bird_indices       = [i for i in train_indices if trainset_raw.targets[i] == bird_id]

target_per_class = max(len(airplane_indices), len(automobile_indices))
bird_needed = target_per_class - len(bird_indices)

if bird_needed > 0:
    reps = math.ceil(bird_needed / max(1, len(bird_indices)))
    bird_extra_indices = (bird_indices * reps)[:bird_needed]
else:
    bird_extra_indices = []

# -----------------------------
# 6) Build balanced training dataset
# -----------------------------
train_airplane_ds   = TransformOnlyImage(Subset(trainset_raw, airplane_indices), base_preprocess)
train_automobile_ds = TransformOnlyImage(Subset(trainset_raw, automobile_indices), base_preprocess)
train_bird_orig_ds  = TransformOnlyImage(Subset(trainset_raw, bird_indices), base_preprocess)

datasets_to_concat = [train_airplane_ds, train_automobile_ds, train_bird_orig_ds]
if len(bird_extra_indices) > 0:
    train_bird_aug_ds = TransformOnlyImage(Subset(trainset_raw, bird_extra_indices), bird_augment)
    datasets_to_concat.append(train_bird_aug_ds)

trainset_balanced = ConcatDataset(datasets_to_concat)

trainloader = DataLoader(trainset_balanced, batch_size=64, shuffle=True)
testset_filtered_v2 = TransformOnlyImage(Subset(testset_raw, test_indices), base_preprocess)
testloader = DataLoader(testset_filtered_v2, batch_size=64, shuffle=True)

# ============================================================
# 7) CHECKS / ASSERTIONS
# ============================================================

print("---- Dataset sizes (raw filtered) ----")
print(f"airplane: {len(airplane_indices)}")
print(f"automobile: {len(automobile_indices)}")
print(f"bird (orig): {len(bird_indices)}")
print(f"bird (extra/aug): {len(bird_extra_indices)}")
print(f"target_per_class: {target_per_class}")
print(f"Total balanced trainset size: {len(trainset_balanced)}")

# Expected: airplane + automobile + bird_total == 3 * target_per_class
bird_total = len(bird_indices) + len(bird_extra_indices)
assert bird_total == target_per_class, (
    f"Bird total ({bird_total}) != target_per_class ({target_per_class}). "
    "Augmented oversampling did not match the majority class."
)
assert len(trainset_balanced) == (len(airplane_indices) + len(automobile_indices) + bird_total), \
    "ConcatDataset length mismatch."

print("\n✅ Class balancing logic: PASSED (bird matches majority).")

# ---- Determinism checks ----
# base_preprocess should be deterministic for same sample index
x1, y1 = train_airplane_ds[0]
x2, y2 = train_airplane_ds[0]
assert y1 == y2, "Label changed between calls (unexpected)."
assert torch.equal(x1, x2), "base_preprocess appears non-deterministic (unexpected)."
print("✅ base_preprocess determinism: PASSED.")

# bird_augment should be stochastic for same underlying sample (with high probability)
if len(bird_extra_indices) > 0:
    # Try a few times to avoid rare equality by chance
    identical = True
    for _ in range(5):
        a1, _ = train_bird_aug_ds[0]
        a2, _ = train_bird_aug_ds[0]
        if not torch.equal(a1, a2):
            identical = False
            break
    assert not identical, "bird_augment seems deterministic (unexpected)."
    print("✅ bird_augment stochasticity: PASSED.")
else:
    print("⚠️ No bird augmentation created (bird_needed <= 0), stochasticity test skipped.")

# ---- Shape / dtype / normalization sanity ----
img, lab = trainset_balanced[0]
assert isinstance(img, torch.Tensor), "Transformed image is not a torch.Tensor."
assert img.dtype == torch.float32, f"Expected float32, got {img.dtype}."
assert img.shape == (3, 224, 224), f"Expected shape (3,224,224), got {tuple(img.shape)}."
# After Normalize(mean=0.5,std=0.5) on [0,1], values are roughly [-1,1], allow some slack for aug/crop/erasing
assert img.min().item() >= -4.0 and img.max().item() <= 4.0, \
    f"Unexpected value range after normalization: min={img.min().item():.3f}, max={img.max().item():.3f}"
print("✅ Tensor shape/dtype/value-range: PASSED.")

# ---- DataLoader batch sanity ----
xb, yb = next(iter(trainloader))
assert xb.shape[1:] == (3, 224, 224), f"Train batch images have wrong shape: {tuple(xb.shape)}"
assert xb.dtype == torch.float32, f"Train batch dtype wrong: {xb.dtype}"
allowed = {airplane_id, automobile_id, bird_id}
assert set(yb.tolist()).issubset(allowed), "Train batch contains labels outside airplane/automobile/bird."
print("✅ Train DataLoader batch sanity: PASSED.")

xt, yt = next(iter(testloader))
assert xt.shape[1:] == (3, 224, 224), f"Test batch images have wrong shape: {tuple(xt.shape)}"
assert xt.dtype == torch.float32, f"Test batch dtype wrong: {xt.dtype}"
assert set(yt.tolist()).issubset(allowed), "Test batch contains labels outside airplane/automobile/bird."
print("✅ Test DataLoader batch sanity: PASSED.")

print("\n🎉 ALL CHECKS PASSED: Data augmentation + oversampling pipeline behaves as intended.")


---- Dataset sizes (raw filtered) ----
airplane: 500
automobile: 500
bird (orig): 200
bird (extra/aug): 300
target_per_class: 500
Total balanced trainset size: 1500

✅ Class balancing logic: PASSED (bird matches majority).
✅ base_preprocess determinism: PASSED.
✅ bird_augment stochasticity: PASSED.
✅ Tensor shape/dtype/value-range: PASSED.
✅ Train DataLoader batch sanity: PASSED.
✅ Test DataLoader batch sanity: PASSED.

🎉 ALL CHECKS PASSED: Data augmentation + oversampling pipeline behaves as intended.


# Fine-Tunning

Now we are going to perform fine-tuning on the two architectures that have been implemented.


# model = # Your model
# model.load_state_dict(vgg19.state_dict())
# model.load_state_dict(alexnet.state_dict())

# We have to modify the Classifier for 3 classes
model = vgg19
num_features = model.classifier[6].in_features
model.classifier[6] = nn.Linear(num_features, 3)
model.to(device)


VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): ReLU(inplace=True)
    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (24): ReLU(inplace=True)
    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (26): ReLU(inplace=True)
    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): ReLU(inplace=True)
    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (33): ReLU(inplace=True)
    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (35): ReLU(inplace=True)
    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=3, bias=True)
  )
)

#@title Loss Function and Optimizer

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.00001)


#@title Training

num_epochs = 3
for epoch in range(num_epochs):
    running_loss = 0.0
    for images, labels in trainloader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()

        # Forward + backward + optimize
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}")


---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
Cell In[12], line 6
      4 for epoch in range(num_epochs):
      5     running_loss = 0.0
----> 6     for images, labels in trainloader:
      7         images, labels = images.to(device), labels.to(device)
      9         optimizer.zero_grad()

NameError: name 'trainloader' is not defined


#@title Test the model

correct = 0
total = 0

with torch.no_grad():
    for images, labels in testloader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f"Accuracy of the model on the test images: {100 * correct / total}%")


import matplotlib.pyplot as plt

model.eval()

# Get a batch of test images and labels
dataiter = iter(testloader)
images, labels = next(dataiter)

# Make predictions
with torch.no_grad():
    outputs = model(images.to(device))
    _, predicted = torch.max(outputs, 1)

# Function to unnormalize and plot an image
def imshow(img):
    img = img / 2 + 0.5  # unnormalize
    plt.imshow(np.transpose(img.numpy(), (1, 2, 0)))

classes = trainset.classes[0:3]
fig = plt.figure(figsize=(10, 4))
for idx in range(3):
    ax = fig.add_subplot(1, 3, idx+1, xticks=[], yticks=[])
    imshow(images[idx])
    ax.set_title(f"True: {classes[labels[idx]]}\nPredicted: {classes[predicted[idx]]}")

plt.show()


# **Questions**

1. Describe the techniques used to balance the dataset.


2. Train both networks with balanced and unbalanced datasets. How does it affect the predictions? Display a confusion matrix for each case.

# =========================
# Q2) Train both networks with balanced and unbalanced datasets + confusion matrices
# =========================

import copy
import numpy as np
import torch
from torch import nn
import torch.optim as optim
import matplotlib.pyplot as plt

# --- Safety checks ---
assert "AlexVGGNet" in globals(), "AlexVGGNet class not found. Run the cell where you define it."
assert "ResInceptionNet" in globals(), "ResInceptionNet class not found. Run the cell where you define it."

assert "trainloader_unbalanced" in globals(), "Run the 'Apply Data Augmentation' cell first."
assert "trainloader_balanced" in globals(), "Run the 'Apply Data Augmentation' cell first."
assert "testloader" in globals(), "Run the 'Apply Data Augmentation' cell first."
assert "NUM_CLASSES" in globals() and NUM_CLASSES == 3, "NUM_CLASSES should be 3."
assert "class_names" in globals() and len(class_names) == 3, "class_names should be the 3 selected classes."

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.backends.cudnn.benchmark = torch.cuda.is_available()  # speed, usually helps; does not change requirements

# --- Metrics helpers ---
def confusion_matrix_np(y_true: np.ndarray, y_pred: np.ndarray, num_classes: int) -> np.ndarray:
    cm = np.zeros((num_classes, num_classes), dtype=np.int64)
    for t, p in zip(y_true, y_pred):
        cm[int(t), int(p)] += 1
    return cm

def metrics_from_cm(cm: np.ndarray):
    total = cm.sum()
    acc = np.trace(cm) / total if total > 0 else 0.0

    per_class = []
    for i in range(cm.shape[0]):
        tp = cm[i, i]
        fn = cm[i, :].sum() - tp
        fp = cm[:, i].sum() - tp
        tn = total - tp - fn - fp

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1        = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0
        spec      = tn / (tn + fp) if (tn + fp) > 0 else 0.0

        per_class.append((precision, recall, spec, f1))

    per_class = np.array(per_class)
    return {
        "accuracy": float(acc),
        "precision_macro": float(per_class[:, 0].mean()),
        "sensitivity_macro": float(per_class[:, 1].mean()),
        "specificity_macro": float(per_class[:, 2].mean()),
        "f1_macro": float(per_class[:, 3].mean()),
    }

def plot_confusion_matrix(cm: np.ndarray, labels: list[str], title: str):
    fig, ax = plt.subplots(figsize=(5.2, 4.6))
    ax.imshow(cm)
    ax.set_title(title)
    ax.set_xlabel("Predicted")
    ax.set_ylabel("True")
    ax.set_xticks(range(len(labels)))
    ax.set_yticks(range(len(labels)))
    ax.set_xticklabels(labels, rotation=35, ha="right")
    ax.set_yticklabels(labels)

    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, str(cm[i, j]), ha="center", va="center")

    plt.tight_layout()
    plt.show()

@torch.no_grad()
def predict(model: nn.Module, loader):
    model.eval()
    y_true, y_pred = [], []
    for x, y in loader:
        x = x.to(device, non_blocking=True)
        y = y.to(device, non_blocking=True)
        logits = model(x)
        preds = logits.argmax(dim=1)
        y_true.append(y.detach().cpu().numpy())
        y_pred.append(preds.detach().cpu().numpy())
    return np.concatenate(y_true), np.concatenate(y_pred)

def train_one_run(
    model: nn.Module,
    trainloader,
    valloader,
    num_epochs: int = 25,
    max_lr: float = 3e-4,
    weight_decay: float = 1e-4,
    label_smoothing: float = 0.10,
    grad_clip: float = 1.0,
    early_stop_patience: int = 6,
    min_epochs: int = 8,
):
    model = model.to(device)

    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)
    optimizer = optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)

    # Cosine LR is stable and usually strong for fine-tuning-like setups
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

    use_amp = torch.cuda.is_available()
    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)

    history = {
        "train_loss": [],
        "val_loss": [],
        "val_accuracy": [],
        "val_sensitivity": [],
        "val_specificity": [],
        "val_f1": [],
        "lr": [],
    }

    best_state = copy.deepcopy(model.state_dict())
    best_score = -1.0
    no_improve = 0

    for epoch in range(num_epochs):
        # ---- Train ----
        model.train()
        running = 0.0

        for x, y in trainloader:
            x = x.to(device, non_blocking=True)
            y = y.to(device, non_blocking=True)

            optimizer.zero_grad(set_to_none=True)

            if use_amp:
                with torch.autocast(device_type="cuda", dtype=torch.float16):
                    logits = model(x)
                    loss = criterion(logits, y)
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                if grad_clip is not None:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
                scaler.step(optimizer)
                scaler.update()
            else:
                logits = model(x)
                loss = criterion(logits, y)
                loss.backward()
                if grad_clip is not None:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
                optimizer.step()

            running += float(loss.item())

        train_loss = running / max(1, len(trainloader))
        history["train_loss"].append(train_loss)

        # ---- Validate ----
        model.eval()
        val_running = 0.0
        y_true, y_pred = [], []

        with torch.no_grad():
            for x, y in valloader:
                x = x.to(device, non_blocking=True)
                y = y.to(device, non_blocking=True)

                if use_amp:
                    with torch.autocast(device_type="cuda", dtype=torch.float16):
                        logits = model(x)
                        vloss = criterion(logits, y)
                else:
                    logits = model(x)
                    vloss = criterion(logits, y)

                val_running += float(vloss.item())
                preds = logits.argmax(dim=1)
                y_true.append(y.detach().cpu().numpy())
                y_pred.append(preds.detach().cpu().numpy())

        val_loss = val_running / max(1, len(valloader))
        y_true = np.concatenate(y_true)
        y_pred = np.concatenate(y_pred)
        cm = confusion_matrix_np(y_true, y_pred, NUM_CLASSES)
        m = metrics_from_cm(cm)

        history["val_loss"].append(val_loss)
        history["val_accuracy"].append(m["accuracy"])
        history["val_sensitivity"].append(m["sensitivity_macro"])
        history["val_specificity"].append(m["specificity_macro"])
        history["val_f1"].append(m["f1_macro"])
        history["lr"].append(float(optimizer.param_groups[0]["lr"]))

        scheduler.step()

        print(
            f"Epoch {epoch+1:02d}/{num_epochs} | "
            f"train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | "
            f"val_acc={m['accuracy']:.4f} | val_f1={m['f1_macro']:.4f} | lr={history['lr'][-1]:.2e}"
        )

        # Early stopping on macro-F1 (robust with imbalance, still improves real accuracy)
        score = m["f1_macro"]
        if (epoch + 1) >= min_epochs:
            if score > best_score + 1e-4:
                best_score = score
                best_state = copy.deepcopy(model.state_dict())
                no_improve = 0
            else:
                no_improve += 1
                if no_improve >= early_stop_patience:
                    print(f"Early stopping at epoch {epoch+1} (best val_f1={best_score:.4f})")
                    break

    model.load_state_dict(best_state)
    return model, history

# -------------------------
# Experiments (2 networks x 2 datasets)
# -------------------------
def build_models_for_3_classes():
    return {
        "AlexVGGNet": AlexVGGNet(num_classes=NUM_CLASSES),
        "ResInceptionNet": ResInceptionNet(num_classes=NUM_CLASSES),
    }

loaders = {
    "unbalanced": trainloader_unbalanced,
    "balanced": trainloader_balanced,
}

# Hyperparameters tuned for accuracy (still respects practice requirements)
NUM_EPOCHS = 25
MAX_LR = 3e-4
WEIGHT_DECAY = 1e-4
LABEL_SMOOTH = 0.10

experiments = {}

for model_name in ["AlexVGGNet", "ResInceptionNet"]:
    for data_name in ["unbalanced", "balanced"]:
        print("\n" + "=" * 80)
        print(f"Training {model_name} on {data_name} dataset")
        print("=" * 80)

        # Reproducibility per run
        set_seed(42)

        model = build_models_for_3_classes()[model_name]

        trained_model, history = train_one_run(
            model=model,
            trainloader=loaders[data_name],
            valloader=testloader,   # as in your current approach
            num_epochs=NUM_EPOCHS,
            max_lr=MAX_LR,
            weight_decay=WEIGHT_DECAY,
            label_smoothing=LABEL_SMOOTH,
            grad_clip=1.0,
            early_stop_patience=6,
            min_epochs=8,
        )

        # Final TEST evaluation + confusion matrix
        y_true, y_pred = predict(trained_model, testloader)
        cm = confusion_matrix_np(y_true, y_pred, NUM_CLASSES)
        m = metrics_from_cm(cm)

        experiments[(model_name, data_name)] = {
            "model": trained_model,
            "history": history,
            "y_true": y_true,
            "y_pred": y_pred,
            "cm": cm,
            "final_metrics": m,
        }

        print(f"\nFinal test metrics | {model_name} | {data_name}: {m}")
        plot_confusion_matrix(cm, class_names, title=f"{model_name} - {data_name} (Confusion Matrix)")


3. Generate graphs to compare the performance of the implemented networks on the different datasets, focusing on metrics such as training/validation loss, sensitivity, specificity, f1-measure ...


# =========================
# Q3) Graphs to compare performance (loss, sensitivity, specificity, f1) across runs
# =========================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

try:
    from IPython.display import display
except Exception:
    display = print

assert "experiments" in globals() and len(experiments) > 0, "No experiments found. Run the Q2 cell first."

def plot_metric_by_model(experiments_dict, metric_key: str, title: str, ylabel: str):
    # Plot per model: balanced vs unbalanced on same axes
    models = sorted({k[0] for k in experiments_dict.keys()})
    for model_name in models:
        plt.figure(figsize=(8.5, 4.2))
        for dataset_name in ["unbalanced", "balanced"]:
            key = (model_name, dataset_name)
            if key not in experiments_dict:
                continue
            hist = experiments_dict[key]["history"]
            if metric_key not in hist:
                continue
            x = np.arange(1, len(hist[metric_key]) + 1)
            plt.plot(x, hist[metric_key], label=f"{dataset_name}")
        plt.xlabel("Epoch")
        plt.ylabel(ylabel)
        plt.title(f"{title} — {model_name}")
        plt.legend()
        plt.tight_layout()
        plt.show()

# 1) Curvas por época
plot_metric_by_model(experiments, "train_loss", "Training Loss", "Loss")
plot_metric_by_model(experiments, "val_loss",   "Validation Loss", "Loss")
plot_metric_by_model(experiments, "val_accuracy",    "Validation Accuracy", "Accuracy")
plot_metric_by_model(experiments, "val_sensitivity", "Validation Sensitivity / Recall (macro)", "Sensitivity (macro)")
plot_metric_by_model(experiments, "val_specificity", "Validation Specificity (macro)", "Specificity (macro)")
plot_metric_by_model(experiments, "val_f1",          "Validation F1 (macro)", "F1 (macro)")

# 2) Tabla resumen con métricas finales (test) + delta balanced - unbalanced
rows = []
for (model_name, data_name), pack in experiments.items():
    m = pack["final_metrics"]
    rows.append({
        "model": model_name,
        "dataset": data_name,
        "accuracy": m["accuracy"],
        "precision_macro": m["precision_macro"],
        "sensitivity_macro": m["sensitivity_macro"],
        "specificity_macro": m["specificity_macro"],
        "f1_macro": m["f1_macro"],
    })

df = pd.DataFrame(rows).sort_values(["model", "dataset"]).reset_index(drop=True)
display(df)

# Delta table
pivot = df.pivot(index="model", columns="dataset")
delta = pd.DataFrame({
    "Δ accuracy (bal - unbal)": pivot["accuracy"]["balanced"] - pivot["accuracy"]["unbalanced"],
    "Δ f1_macro (bal - unbal)": pivot["f1_macro"]["balanced"] - pivot["f1_macro"]["unbalanced"],
    "Δ sensitivity (bal - unbal)": pivot["sensitivity_macro"]["balanced"] - pivot["sensitivity_macro"]["unbalanced"],
    "Δ specificity (bal - unbal)": pivot["specificity_macro"]["balanced"] - pivot["specificity_macro"]["unbalanced"],
})
display(delta)

# 3) Bar charts finales (métricas test)
def barplot_metric_grouped(df_in: pd.DataFrame, metric: str, title: str, ylabel: str):
    models = df_in["model"].unique().tolist()
    x = np.arange(len(models))
    width = 0.35

    vals_unbal = []
    vals_bal = []
    for m in models:
        sub = df_in[df_in["model"] == m].set_index("dataset")
        vals_unbal.append(float(sub.loc["unbalanced", metric]))
        vals_bal.append(float(sub.loc["balanced", metric]))

    plt.figure(figsize=(8.8, 4.2))
    plt.bar(x - width/2, vals_unbal, width, label="unbalanced")
    plt.bar(x + width/2, vals_bal,   width, label="balanced")
    plt.xticks(x, models, rotation=0)
    plt.ylabel(ylabel)
    plt.title(title)
    plt.legend()
    plt.tight_layout()
    plt.show()

barplot_metric_grouped(df, "accuracy",          "Final TEST Accuracy (balanced vs unbalanced)", "Accuracy")
barplot_metric_grouped(df, "sensitivity_macro", "Final TEST Sensitivity/Recall (macro)", "Sensitivity (macro)")
barplot_metric_grouped(df, "specificity_macro", "Final TEST Specificity (macro)", "Specificity (macro)")
barplot_metric_grouped(df, "f1_macro",          "Final TEST F1 (macro)", "F1 (macro)")